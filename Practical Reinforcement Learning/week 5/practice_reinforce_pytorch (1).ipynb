{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practice_reinforce_pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFVGaPLN1xra",
        "colab_type": "text"
      },
      "source": [
        "# REINFORCE in PyTorch\n",
        "\n",
        "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LHBCOGY1xrd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPoc-2b41xrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vkMmiuX1xrv",
        "colab_type": "text"
      },
      "source": [
        "A caveat: we have received reports that the following cell may crash with `NameError: name 'base' is not defined`. The [suggested workaround](https://www.coursera.org/learn/practical-rl/discussions/all/threads/N2Pw652iEemRYQ6W2GuqHg/replies/te3HpQwOQ62tx6UMDoOt2Q/comments/o08gTqelT9KPIE6npX_S3A) is to install `gym==0.14.0` and `pyglet==1.3.2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5koc2bw1xrw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "a7c0d8c2-f83a-4315-93ea-d786ea73ca94"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, '_max_episode_steps'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd9e9f231d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eZ_zMap1xr6",
        "colab_type": "text"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAIiGqQp1xr7",
        "colab_type": "text"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNrv6-5a1xr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IRMtnUa1xsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a simple neural network that predicts policy logits. \n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "model = nn.Sequential(\n",
        "  nn.Linear(state_dim[0], 24),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(24, 32),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(32, n_actions)\n",
        ")\n",
        "\n",
        "  \n",
        "# print(summary(model, state_dim))\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixkWmOKa1xsK",
        "colab_type": "text"
      },
      "source": [
        "#### Predict function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itpKA8a81xsL",
        "colab_type": "text"
      },
      "source": [
        "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
        "So, here gradient calculation is not needed.\n",
        "<br>\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "<br>\n",
        "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
        "<br>\n",
        "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
        "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "<br>\n",
        "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPihW2KK1xsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "def predict_probs(states):\n",
        "    \"\"\" \n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "      actions_prob = model(states)\n",
        "      logits = F.softmax(actions_prob)\n",
        "    \n",
        "    return logits.numpy()"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PaK_lj_1xsY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "78a83198-8056-4421-9c1a-b0af8c6e5a1c"
      },
      "source": [
        "test_states = np.array([env.reset() for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MymNWmV91xsh",
        "colab_type": "text"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9Sykc9V1xsi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\" \n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice(n_actions, 1, p=action_probs)[0]\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9xdrE7t1xsy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "2d597ab7-d862-47d0-c4b8-eb84f5627310"
      },
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAx4nOhi1xtO",
        "colab_type": "text"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVOUCUqo1xtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session \n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "    \n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    cumm_rewards = np.zeros_like(rewards, dtype = np.float32)\n",
        "    cumm_rewards[-1] = rewards[-1]\n",
        "    for i in range(len(rewards)-2,-1,-1):\n",
        "        cumm_rewards[i] = rewards[i] + gamma * cumm_rewards[i+1]\n",
        "    # print(cumm_rewards)\n",
        "    # print(rewards)\n",
        "    return cumm_rewards"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWIi6rL91xth",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "403175ee-dd1a-4533-c6e5-af151252d162"
      },
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks good!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac9Rpyli1xts",
        "colab_type": "text"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZwfEjHQ1xtu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8VtHBxJ1xt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # cast everything into torch tensors\n",
        "    optimizer.zero_grad()\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        "    # print(log_probs)\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
        "    \n",
        "    # print(log_probs_for_actions)\n",
        "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
        "    J = torch.mean(log_probs_for_actions*cumulative_returns)\n",
        "    entropy = -torch.mean(probs*log_probs)\n",
        "    loss =  - J - entropy_coef*entropy\n",
        "    # print(loss)\n",
        "    # # Gradient descent step\n",
        "    # <YOUR CODE>\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38X0-ziQ1xuF",
        "colab_type": "text"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xHJ7pwz1xuK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "e32461a9-2279-43c8-8eef-6d827100293e"
      },
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
        "    \n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    \n",
        "    if np.mean(rewards) > 300:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "mean reward:21.470\n",
            "mean reward:23.150\n",
            "mean reward:24.190\n",
            "mean reward:34.390\n",
            "mean reward:48.730\n",
            "mean reward:86.200\n",
            "mean reward:130.660\n",
            "mean reward:149.350\n",
            "mean reward:146.360\n",
            "mean reward:176.500\n",
            "mean reward:397.620\n",
            "You Win!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ2HOTFC1xub",
        "colab_type": "text"
      },
      "source": [
        "### Results & video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Haqb4c01xud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "e58c8161-e329-40dc-bfdf-52cfce6aabc9"
      },
      "source": [
        "# Record sessions\n",
        "\n",
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
        "    sessions = [generate_session(env_monitor) for _ in range(100)]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERZhQYLW1xuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBnrTwWe1xuy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "5e115b52-ba98-49dd-95b3-018c0068b3d6"
      },
      "source": [
        "from submit import submit_cartpole\n",
        "submit_cartpole(generate_session, 'ayu.150399@gmail.com', 'FEV8HWELwQ0QhBds')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Your average reward is 304.02 over 100 episodes\n",
            "Submitted to Coursera platform. See results on assignment page!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKXOJyP41xu5",
        "colab_type": "text"
      },
      "source": [
        "That's all, thank you for your attention!\n",
        "\n",
        "Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."
      ]
    }
  ]
}